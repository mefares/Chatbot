import os

import base64

import gc

import random

import tempfile

import time

import uuid

import sqlite3

from huggingface_hub import configure_http_backend

import requests

from llama_index.embeddings.huggingface import HuggingFaceEmbedding  # import model from huggingface 

from llama_index.core import Settings

from llama_index.core import VectorStoreIndex, ServiceContext, load_index_from_storage, StorageContext, SimpleDirectoryReader # indexing the documents # reading pdf , doc ,...

from llama_index.core import PromptTemplate

from llama_index.llms.openai import OpenAI

from llama_index.llms.llama_cpp import LlamaCPP  # loading llm using llamacpp

from llama_cpp import Llama

from llama_index.llms.llama_cpp.llama_utils import (

    messages_to_prompt,

    completion_to_prompt,

)


from PIL import Image

import streamlit as st

from huggingface_hub import configure_http_backend

import requests
 
def backend_factory() -> requests.Session:  # SSL certification 

    session = requests.Session()

    session.verify = False

    return session
 
configure_http_backend(backend_factory=backend_factory)
 # Initializing session state 
if "id" not in st.session_state:

    st.session_state.id = uuid.uuid4()

    st.session_state.file_cache = {}

    st.session_state.loaded_history = []

    st.session_state.selected_history = None
 
session_id = st.session_state.id

client = None
 
def reset_chat(): # Reseting the chat 

    st.session_state.messages = []

    st.session_state.context = None

    st.session_state.loaded_history = []

    st.session_state.selected_history = None

    gc.collect()
 
llm = LlamaCPP(                           # Loading the llm parameters 

    model_path="C:\Fares\Phi-3-medium-128k-instruct-Q4_K_M.gguf",

    temperature=0.1,

    max_new_tokens=1024,

    context_window=10000,

    generate_kwargs={},

    model_kwargs={"n_gpu_layers": 0},

    messages_to_prompt=messages_to_prompt,

    completion_to_prompt=completion_to_prompt,

    verbose=True,

)
 
image = Image.open('src/211110_EXE_LOGO_ODDO_HORIZONTAL_RVB.png')

st.image(image, width=200)
 
embed_model = HuggingFaceEmbedding(

    model_name="BAAI/bge-large-en-v1.5"

)

Settings.llm = llm

Settings.embed_model = embed_model
 
storage_context = StorageContext.from_defaults(persist_dir="./Storage")

index = load_index_from_storage(storage_context, embed_model=embed_model)
 
def init_db(): # Creating database 

    conn = sqlite3.connect('chat_history.db')

    c = conn.cursor()

    c.execute('''CREATE TABLE IF NOT EXISTS chat_history (session_id TEXT, role TEXT, content TEXT)''')

    conn.commit()

    conn.close()
 
init_db()
 
def save_to_db(session_id, role, content): # Saving questions/answers for chat_history 

    conn = sqlite3.connect('chat_history.db')

    c = conn.cursor()

    c.execute("INSERT INTO chat_history (session_id, role, content) VALUES (?, ?, ?)", (str(session_id), role, content))

    conn.commit()

    conn.close()
 
def load_chat_history(): # Loading chathistory from the database 

    conn = sqlite3.connect('chat_history.db')

    c = conn.cursor()

    c.execute("SELECT DISTINCT session_id FROM chat_history")

    session_ids = c.fetchall()

    session_histories = {}

    for session_id in session_ids:

        c.execute("SELECT role, content FROM chat_history WHERE session_id=?", (session_id[0],))

        session_histories[session_id[0]] = c.fetchall()

    conn.close()

    return session_histories
 
st.sidebar.header("Chat History")

chat_histories = load_chat_history()
 
for session_id, history in chat_histories.items():

    content = history[0]

    with st.sidebar: # creating a sidebar 

        # for role, content in history:

        #     st.write(f"{role}: {content}")
        

          if st.button(f"{content[1]}",key=session_id):

            st.session_state.selected_history = history # Creating buttons , if the button is clicked the session loaded  
 
qa_prompt_tmpl_str = (

    "Context information is below.\n"

    "---------------------\n"

    "{context_str}\n"

    "---------------------\n"

    "Given the context information above I want you to think step by step to answer the query in a detailed manner, in case you don't know the answer say 'I don't know. Provide answer only in French'.\n"

    "Query: {query_str}\n"

    "Answer: "

)

qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)

query_engine = index.as_query_engine(similarity_top_k=2, llm=llm) # Changing the top k similarity and setting the query_engine 

query_engine.update_prompts({"response_synthesizer:text_qa_template": qa_prompt_tmpl})
 
# mistral_model_path = "C:\DASAPE\openhermes-2.5-mistral-7b.Q2_K.gguf"

# llama_model = Llama(model_path=mistral_model_path)
 
col1, col2 = st.columns([6, 1])

with col1:

    st.header("RFP OBAS")

with col2:

    st.button("Clear â†º", on_click=reset_chat)
 
if "messages" not in st.session_state:

    reset_chat()
 
# Load selected chat history into the main chat area

if st.session_state.selected_history:

    for role, content in st.session_state.selected_history:

        st.session_state.messages.append({"role": role, "content": content})

    st.session_state.selected_history = None  # Clear after loading
 
# Render all chat messages, including the loaded history

for message in st.session_state.messages:

    with st.chat_message(message["role"]):

        st.markdown(message["content"])
 
if prompt := st.chat_input("What's up?"):

    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):

        st.markdown(prompt)
 
    with st.chat_message("assistant"):

        response = query_engine.query(prompt)

        response_1 = response.response
 
        list_file = []

        spe_char_to_remove = ["[/INST]", "[/SYS]", "[/ASSISTANT]", "[/END]"]

        for character in spe_char_to_remove:

            response_1 = response_1.replace(character, ' ')
 
        for n in response.source_nodes:

            list_file.append(n.metadata["file_name"])
 
        my_set = set(list_file)

        new_list_file = list(my_set)

        for i in new_list_file:

            response_1 += f"\nSource: {i} "
 
        st.markdown(response_1)

        for i, node in enumerate(response.source_nodes): # Displaying text chunk 

            with st.expander(f"Source: {node.metadata['file_name']}"):

                st.markdown(

                    f"""
                         <div style="font-size:16px; font-family:sans-serif;">

                        Contenu : {node.text}
                            </div>

                    """,

                    unsafe_allow_html=True

                )
 
    st.session_state.messages.append({"role": "assistant", "content": response_1})
 
    save_to_db(session_id, "user", prompt)

    save_to_db(session_id, "assistant", response_1)
 
